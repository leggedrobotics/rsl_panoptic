# Supervised Training, Evaluation and Inference
Supervised Training, Evaluation and Inference is based on the [Mask2Former](https://github.com/facebookresearch/Mask2Former) framework. 
Training and Inference Scripts are adapted from the official [Mask2Former Repository](https://github.com/facebookresearch/Mask2Former/blob/main/GETTING_STARTED.md)

## Inference Procedure
Here an example with a pre-trained model is shown. The steps can be adapted accordingly to use an own model. 

1. Pick a model from the [model zoo](../../MODEL_ZOO.md) and its config file from [models](../models/) for example, `models/mask2former_backbone/swin_t/maskformer2_swin_tiny_bs16_50ep.yaml`.
2. The provided `m2f_demo.py`  will run the inference and show visualizations in an OpenCV window if a single file is selected or save the predictions in the defined directory. Run it with:
```
cd mask2former/m2f_deploy/
python m2f_demo.py --config-file ../models/mask2former_backbone/swin_t/maskformer2_swin_tiny_bs16_50ep.yaml \
  --input input1.jpg input2.jpg \
  [--other-optins]
  --opts MODEL.WEIGHTS /path/to/checkpoint_file
```
The configs are made for training, therefore we need to specify `MODEL.WEIGHTS` to enable inference for an already trained model.

For details of the command line arguments, see `m2f_demo.py -h` or look at its source code
to understand its behavior. Some common arguments are:
* To run __on your webcam__, replace `--input files` with `--webcam`.
* To run __on a video__, replace `--input files` with `--video-input video.mp4`.
* To run __on cpu__, add `MODEL.DEVICE cpu` after `--opts`.
* To save outputs to a directory (for images) or a file (for webcam or video), use `--output`.


## Training Procedure
To train a model with "m2f_train.py", first
setup the corresponding datasets following
[Data Description](../data/README.md).

**Purely Supervised Case**:

To train the model run the following command:
```
python3 m2f_train.py --num-gpus 8 \
  --config-file CONFIG-YAML-PATH 
```

Pre-trained weights for the entire M2F network can be defined in the config file. 

**MAE Pre-trained Model**:

The encoder weights generated by the MAE pre-training are not saved in the same format as the weights saved by M2F. 
Thus, definition in the config does not work and argument `--mae-pretrain` has been introduced. 
To execute training run:

```
python3 m2f_train.py --num-gpus 8 \
  --config-file CONFIG-YAML-PATH \
  --mae-pretrain MODEL-WEIGHTS-PATH \
  --resume 
```

**General Information**

The included configs are made for 1-GPU training. 
Since ADAMW optimizer is used, it is not clear how to scale learning rate with batch size.
To train on multiple GPUs, you need to figure out learning rate and batch size by yourself:
```
python m2f_train.py \
  --config-file ../models/mask2former_backbone/swin_t/maskformer2_swin_tiny_bs16_50ep.yaml \
  --num-gpus 1 SOLVER.IMS_PER_BATCH SET_TO_SOME_REASONABLE_VALUE SOLVER.BASE_LR SET_TO_SOME_REASONABLE_VALUE
```

If for the evaluation just the metric scores (i.e. Panoptic, Semantic and Recognition Quality) are sufficient, the performance is directly given using the key `--eval-only` together with the model weights: 

```
python m2f_train.py \
  --config-file ../models/mask2former_backbone/swin_t/maskformer2_swin_tiny_bs16_50ep.yaml \
  --eval-only MODEL.WEIGHTS /path/to/checkpoint_file
```

To access all options, see `python m2f_train.py -h`.

## Evaluation

To evaluate the model performance the Panoptic, Semantic and Recognition Quality is used, as introduced by [Kirillov et al.](https://arxiv.org/pdf/1801.00868.pdf). 
Scripts are provided to provide the scores and plot them over the training progress. Two kind of scripts are provided: 

- `m2f_eval.py`: Evaluation of a single model. For all intermediate models (iteration steps between each model saving can be defined in the configs) the metric scores on (multiple) dataset(s) are determined. The plots include the different datasets s.t. a comparison between them is possible. The learning process is plotted and for the final model, the visual predictions for the test dataset are generated.  For detailed information about the arguments call `m2f_eval.py -h`
- `m2f_eval_multi_model`: Comparison between different models. Training progress of the evaluation metrics of different models is plotted. Please note that the single evaluation scripts has to be executed for each of these models beforehand. For detailed information about the arguments call `m2f_eval_multi_model.py -h` 

*Remark*: Please note that both scripts are fitted for the conducted investigation. It might be necessary to change e.g. the names of the datasets in the case own datasets are used. 